{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Spam Classifier with Keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeosol/ai-api/blob/main/Spam_Classifier_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o53USBMWhsEU"
      },
      "source": [
        "# Build a Spam Classifier with Keras\n",
        "With deep learning and AI, handling spam content has gotten easier and easier. Over time (and with the aid of direct user feedback) our spam classifier will rarely produce erroneous results. \n",
        "\n",
        "This is the first part of a multi-part series covering how to:\n",
        "\n",
        "- Build an AI Model (this one)\n",
        "- Integrate a NoSQL Database (inference result storing)\n",
        "- Deploy an AI Model into Production\n",
        "\n",
        "### Prerequisites\n",
        "- Prepare your dataset using [this notebook](https://github.com/codingforentrepreneurs/AI-as-an-API/blob/main/guides/spam-classifier/1%20-%20Prepare%20the%20AI%20Spam%20Classifier%20Dataset.ipynb) .\n",
        "- Convert your dataset into trainable vectors in [this notebook](https://github.com/codingforentrepreneurs/AI-as-an-API/blob/main/guides/spam-classifier/2%20-%20Convert%20Dataset%20into%20Vectors.ipynb) (Either way, this notebook will run this step for us).\n",
        "\n",
        "\n",
        "### Running this notebook:\n",
        "- Recommended: Use [Colab](https://colab.research.google.com/github/codingforentrepreneurs/AI-as-an-API/blob/main/guides/spam-classifier/Spam_Classifier_with_Keras.ipynb) as it offers free GPUs for training models. [Launch this notebook here]([Colab](https://colab.research.google.com/github/codingforentrepreneurs/AI-as-an-API/blob/main/guides/spam-classifier/Spam_Classifier_with_Keras.ipynb))\n",
        "- Fork [the AI as an API repo](https://github.com/codingforentrepreneurs/AI-as-an-API) and run `guides/spam-classifier/Spam_Classifier_with_Keras.ipynb` whenever you'd like.\n",
        "\n",
        "This notebook is brought to in you in partnership with [DataStax](https://dtsx.io/3nRWZEG)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLgfRoHiy0NG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77294ff4-5482-42ec-cbd4-0c648425092b"
      },
      "source": [
        "#!sudo update-alternatives --config python3\n",
        "#!python3 --version\n",
        "#!sudo update-alternatives --set python /usr/bin/python3.9\n",
        "!pip install boto3\n",
        "!pip install pandas tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.19.5-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting botocore<1.23.0,>=1.22.5\n",
            "  Downloading botocore-1.22.5-py3-none-any.whl (8.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.1 MB 41.7 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.3 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.23.0,>=1.22.5->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.23.0,>=1.22.5->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.19.5 botocore-1.22.5 jmespath-0.10.0 s3transfer-0.5.0 urllib3-1.26.7\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.41.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.6.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Installing collected packages: urllib3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.7\n",
            "    Uninstalling urllib3-1.26.7:\n",
            "      Successfully uninstalled urllib3-1.26.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJqrnVLmTxfG"
      },
      "source": [
        "import boto3\n",
        "import os\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BSsnKlpU_pB"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVD_ZKwBOYSq"
      },
      "source": [
        "EXPORT_DIR = pathlib.Path('/datasets/exports/')\n",
        "GUIDES_DIR = pathlib.Path(\"/guides/spam-classifier/\")\n",
        "DATASET_CSV_PATH = EXPORT_DIR / 'spam-dataset.csv'\n",
        "TRAINING_DATA_PATH = EXPORT_DIR / 'spam-training-data.pkl'\n",
        "PART_TWO_GUIDE_PATH = GUIDES_DIR / \"2 - Convert Dataset into Vectors.ipynb\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZPDBgY4cApj"
      },
      "source": [
        "## Prepare Dataset\n",
        "\n",
        "Creating a dataset rarely happens next to where you run the training. The below cells are a method for us to extract the needed data to perform training against."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1_igE41T2GD",
        "outputId": "f6369e25-4ac2-41da-c419-b4e603e770d0"
      },
      "source": [
        "!mkdir -p \"$EXPORT_DIR\"\n",
        "!mkdir -p \"$GUIDES_DIR\"\n",
        "!curl \"https://raw.githubusercontent.com/jeosol/ai-api/main/datasets/exports/spam-dataset.csv\" -o \"$DATASET_CSV_PATH\"\n",
        "!curl \"https://raw.githubusercontent.com/jeosol/ai-api/main/notebooks/5%20-%20Convert%20Dataset%20into%20Vectors,%20Split%20&%20Export.ipynb\" -o \"$PART_TWO_GUIDE_PATH\"\n",
        "#!curl \"https://raw.githubusercontent.com/codingforentrepreneurs/AI-as-an-API/main/datasets/exports/spam-dataset.csv\" -o \"$DATASET_CSV_PATH\"\n",
        "#!curl \"https://raw.githubusercontent.com/codingforentrepreneurs/AI-as-an-API/main/guides/spam-classifier/2%20-%20Convert%20Dataset%20into%20Vectors.ipynb\" -o \"$PART_TWO_GUIDE_PATH\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  741k  100  741k    0     0  2362k      0 --:--:-- --:--:-- --:--:-- 2362k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  124k  100  124k    0     0   567k      0 --:--:-- --:--:-- --:--:--  565k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RYybCsJczqn"
      },
      "source": [
        "Let's review our extracted dataset which combines two different spam datasets as outlined in [this notebook](https://github.com/codingforentrepreneurs/AI-as-an-API/blob/main/guides/spam-classifier/1%20-%20Prepare%20the%20AI%20Spam%20Classifier%20Dataset.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86o_o7iGEAtj",
        "outputId": "0de828ca-f564-4776-8430-4ad630f93e4a"
      },
      "source": [
        "print(PART_TWO_GUIDE_PATH)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/guides/spam-classifier/2 - Convert Dataset into Vectors.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "OBbbRHAITzq0",
        "outputId": "6556b96e-8e01-47c7-cd9e-85eab4e0ce9c"
      },
      "source": [
        "df = pd.read_csv(DATASET_CSV_PATH, error_bad_lines=False)\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>sms-spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>sms-spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>sms-spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>sms-spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>sms-spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                               text    source\n",
              "0   ham  Go until jurong point, crazy.. Available only ...  sms-spam\n",
              "1   ham                      Ok lar... Joking wif u oni...  sms-spam\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...  sms-spam\n",
              "3   ham  U dun say so early hor... U c already then say...  sms-spam\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...  sms-spam"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWH9NM8KcsSY"
      },
      "source": [
        "In [this notebook](https://github.com/codingforentrepreneurs/AI-as-an-API/blob/main/guides/spam-classifier/2%20-%20Convert%20Dataset%20into%20Vectors.ipynb) we prepare our dataset (`spam-dataset.csv`) to be fully ready for training on a model. Below is a command to run that notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI3KbxGIMNrL",
        "outputId": "4ebdc592-063c-4f25-c557-94b535b3f673"
      },
      "source": [
        "%run \"$PART_TWO_GUIDE_PATH\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W60Gz74_dB4j"
      },
      "source": [
        "Extract prepared training dataset results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2ABatrpKmW6"
      },
      "source": [
        "data = {}\n",
        "\n",
        "with open(TRAINING_DATA_PATH, 'rb') as f:\n",
        "    data = pickle.load(f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuXqr1HuY2Ie",
        "outputId": "78c9f3f7-97c6-4982-a00b-33b58238bcdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'X_test': array([[  0,   0,   0, ...,  11,  70,  19],\n",
              "        [  0,   0,   0, ...,   7, 165,  25],\n",
              "        [  0,   0,   0, ...,   0,   0,   0],\n",
              "        ...,\n",
              "        [  0,   0,   0, ..., 206,  56,   5],\n",
              "        [  0,   0,   0, ...,  16,  73,  19],\n",
              "        [  0,   0,   0, ...,  26, 104, 106]], dtype=int32),\n",
              " 'X_train': array([[  0,   0,   0, ...,  77, 263, 218],\n",
              "        [  0,   0,   0, ..., 261,  12,  18],\n",
              "        [  0,   0,   0, ...,  12,  46, 245],\n",
              "        ...,\n",
              "        [  0,   0,   0, ...,   0,   0,   1],\n",
              "        [  0,   0,   0, ...,  30, 182,   9],\n",
              "        [  0,   0,   0, ...,   0,   3, 156]], dtype=int32),\n",
              " 'labels_legend_inverted': {'0': 'ham', '1': 'spam'},\n",
              " 'legend': {'ham': 0, 'spam': 1},\n",
              " 'max_sequence': 280,\n",
              " 'max_words': 280,\n",
              " 'tokenizer': <keras_preprocessing.text.Tokenizer at 0x7f7e304e0b10>,\n",
              " 'y_test': array([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        ...,\n",
              "        [0., 1.],\n",
              "        [0., 1.],\n",
              "        [0., 1.]], dtype=float32),\n",
              " 'y_train': array([[1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        ...,\n",
              "        [1., 0.],\n",
              "        [1., 0.],\n",
              "        [1., 0.]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVcF78xedHkb"
      },
      "source": [
        "> While the above code uses `pickle` to load in data, this data is actually exported via `pickle` when we execute the `%run` only a few steps ago. Since `pickle` can be unsafe to use from third-party downloaded data, we actually generate (again using `%run`) this pickle data and therefore is safe to use -- it's never downloaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47BJJRESdpV5"
      },
      "source": [
        "## Transform Extracted Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SPvQSPlPFuo"
      },
      "source": [
        "X_test = data['X_test']\n",
        "X_train = data['X_train']\n",
        "y_test = data['y_test']\n",
        "y_train = data['y_train']\n",
        "labels_legend_inverted = data['labels_legend_inverted']\n",
        "legend = data['legend']\n",
        "max_sequence = data['max_sequence']\n",
        "max_words = data['max_words']\n",
        "tokenizer = data['tokenizer']"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9S87BLRdvmo"
      },
      "source": [
        "## Create our LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ScyC-JIU81a",
        "outputId": "235338c6-ac30-4093-bc3e-4f1564f52e86"
      },
      "source": [
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "# Need to understand the neural network model here; Add a discussion in a cell above for the architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(MAX_NUM_WORDS, embed_dim, input_length=X_train.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3)) # LSTM used for text related data, dropout and recurrent dropout (study these)\n",
        "model.add(Dense(2, activation='softmax')) # specify the number of classes here, get a softmax corresponding to probabilities of each class\n",
        "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 280, 128)          35840     \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 280, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 2)                 394       \n",
            "=================================================================\n",
            "Total params: 291,034\n",
            "Trainable params: 291,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liwS1atYVzj0",
        "outputId": "1264c26f-fd12-4887-8df1-a787684648b2"
      },
      "source": [
        "# split the data to do validation and testing later on. Current format uses the test data as validation\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, verbose=1, epochs=epochs)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "158/158 [==============================] - 329s 2s/step - loss: 0.2862 - accuracy: 0.8868 - val_loss: 0.1391 - val_accuracy: 0.9509\n",
            "Epoch 2/5\n",
            "158/158 [==============================] - 318s 2s/step - loss: 0.1403 - accuracy: 0.9552 - val_loss: 0.1381 - val_accuracy: 0.9561\n",
            "Epoch 3/5\n",
            "158/158 [==============================] - 323s 2s/step - loss: 0.1340 - accuracy: 0.9554 - val_loss: 0.1287 - val_accuracy: 0.9565\n",
            "Epoch 4/5\n",
            "158/158 [==============================] - 319s 2s/step - loss: 0.1252 - accuracy: 0.9593 - val_loss: 0.1391 - val_accuracy: 0.9553\n",
            "Epoch 5/5\n",
            "158/158 [==============================] - 322s 2s/step - loss: 0.1199 - accuracy: 0.9627 - val_loss: 0.1261 - val_accuracy: 0.9610\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7db0accbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VPFG3ZSV3Pr"
      },
      "source": [
        "MODEL_EXPORT_PATH = EXPORT_DIR / 'spam-model.h5'\n",
        "model.save(str(MODEL_EXPORT_PATH))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjsaECU61_p9",
        "outputId": "984b0f54-c1b0-4976-a131-4ff0fc626dd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f4ce02d67d0>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha2sP2r0ebDi"
      },
      "source": [
        "## Predict new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gfbt6BQoV6_k"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict(text_str, max_words=280, max_sequence = 280, tokenizer=None):\n",
        "  if not tokenizer:\n",
        "    return None\n",
        "  sequences = tokenizer.texts_to_sequences([text_str])\n",
        "  x_input = pad_sequences(sequences, maxlen=max_sequence)\n",
        "  y_output = model.predict(x_input)\n",
        "  top_y_index = np.argmax(y_output)\n",
        "  preds = y_output[top_y_index]      \n",
        "  preds = model.predict(x_input)\n",
        "  labeled_preds = [{f\"{labels_legend_inverted[str(i)]}\": x} for i, x in enumerate(preds)]\n",
        "  return labeled_preds"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_Yy6wdXe7R6",
        "outputId": "427c2bce-20bb-4aba-8ba6-488a681b3670"
      },
      "source": [
        "predict(\"Hello world\", max_words=max_words, max_sequence=max_sequence, tokenizer=tokenizer)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'ham': array([0.94839007, 0.05160988], dtype=float32)}]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WexqrUW1m9IK"
      },
      "source": [
        "## Exporting Tokenizer & Metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpQqUbFvm3CU",
        "outputId": "0fb03c88-281a-488d-d1a2-65701245db05"
      },
      "source": [
        "import json\n",
        "metadata = {\n",
        "    \"labels_legend_inverted\": labels_legend_inverted,\n",
        "    \"legend\": legend,\n",
        "    \"max_sequence\": max_sequence,\n",
        "    \"max_words\": max_words,\n",
        "}\n",
        "\n",
        "METADATA_EXPORT_PATH = EXPORT_DIR / 'spam-classifer-metadata.json'\n",
        "METADATA_EXPORT_PATH.write_text(json.dumps(metadata, indent=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "187"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU8LwhYHniqu",
        "outputId": "8dfe2b7a-c868-4932-b300-62a37074b98e"
      },
      "source": [
        "tokenizer_as_json = tokenizer.to_json()\n",
        "\n",
        "TOKENIZER_EXPORT_PATH = EXPORT_DIR / 'spam-classifer-tokenizer.json'\n",
        "TOKENIZER_EXPORT_PATH.write_text(tokenizer_as_json)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "827612"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyAtmJJgoFyc"
      },
      "source": [
        "We can load `tokenizer_as_json` with `tensorflow.keras.preprocessing.text.tokenizer_from_json`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UMBCrswmTt-"
      },
      "source": [
        "## Upload Model, Tokenizer, & Metadata to Object Storage\n",
        "\n",
        "Object Storage options include:\n",
        "\n",
        "- AWS S3\n",
        "- Linode Object Storage\n",
        "- DigitalOcean Spaces\n",
        "\n",
        "\n",
        "All three of these options can use `boto3`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg7rXk1-yh3T"
      },
      "source": [
        "# AWS S3 Config\n",
        "ACCESS_KEY = \"<your_aws_iam_key_id>\"\n",
        "SECRET_KEY = \"<your_aws_iam_secret_key>\"\n",
        "\n",
        "# You should not have to set this\n",
        "ENDPOINT = None\n",
        "\n",
        "# Your s3-bucket region\n",
        "REGION = 'us-west-1'\n",
        "\n",
        "BUCKET_NAME = '<your_s3_bucket_name>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM_5XNwf2vat"
      },
      "source": [
        "#### Linode Object Storage Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVw0G1szyc3T"
      },
      "source": [
        "ACCESS_KEY = \"<your_linode_object_storage_access_key>\"\n",
        "SECRET_KEY = \"<your_linode_object_storage_secret_key>\"\n",
        "\n",
        "# Object Storage Endpoint URL\n",
        "ENDPOINT = \"https://cfe3.us-east-1.linodeobjects.com\"\n",
        "\n",
        "# Object Storage Endpoint Region (also in your endpoint url)\n",
        "REGION = 'us-east-1'\n",
        "\n",
        "# Set this to a valid slug (without a \"/\" )\n",
        "BUCKET_NAME = 'datasets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4b8n5Np2sLU"
      },
      "source": [
        "#### DigitalOcean Spaces Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wQWHyZyyZto"
      },
      "source": [
        "ACCESS_KEY = \"<your_do_spaces_access_key>\"\n",
        "SECRET_KEY = \"<your_do_spaces_secret_key>\"\n",
        "\n",
        "# Space Endpoint URL\n",
        "ENDPOINT = \"https://ai-cfe-1.nyc3.digitaloceanspaces.com\"\n",
        "\n",
        "# Space Region (also in your endpoint url)\n",
        "REGION = 'nyc3'\n",
        "\n",
        "# Set this to a valid slug (without a \"/\" )\n",
        "BUCKET_NAME = 'datasets'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcNiEDz4yczw"
      },
      "source": [
        "## Perform Upload with Boto3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anHupbs23GwB"
      },
      "source": [
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = ACCESS_KEY\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = SECRET_KEY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gECGbRgzcAv"
      },
      "source": [
        "# Upload paths \n",
        "MODEL_KEY_NAME = f\"exports/spam-sms/{MODEL_EXPORT_PATH.name}\"\n",
        "TOKENIZER_KEY_NAME = f\"exports/spam-sms/{TOKENIZER_EXPORT_PATH.name}\"\n",
        "METADATA_KEY_NAME = f\"exports/spam-sms/{METADATA_EXPORT_PATH.name}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJvboCTTwz_A"
      },
      "source": [
        "session = boto3.session.Session()\n",
        "client = session.client('s3', region_name=REGION, endpoint_url=ENDPOINT)\n",
        "client.upload_file(str(MODEL_EXPORT_PATH), BUCKET_NAME,  MODEL_KEY_NAME) \n",
        "client.upload_file(str(TOKENIZER_EXPORT_PATH), BUCKET_NAME,  TOKENIZER_KEY_NAME) \n",
        "client.upload_file(str(METADATA_EXPORT_PATH), BUCKET_NAME,  METADATA_KEY_NAME)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYWesuwOvR95"
      },
      "source": [
        "client.download_file(BUCKET_NAME, MODEL_KEY_NAME, pathlib.Path(MODEL_KEY_NAME).name)\n",
        "client.download_file(BUCKET_NAME, TOKENIZER_KEY_NAME, pathlib.Path(TOKENIZER_KEY_NAME).name)\n",
        "client.download_file(BUCKET_NAME, METADATA_KEY_NAME, pathlib.Path(METADATA_KEY_NAME).name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqkNfeCjBTC-"
      },
      "source": [
        "## Implement an AI Model Download Pipeline\n",
        "In [this blog post](https://www.codingforentrepreneurs.com/blog/ai-model-download-pipeline) I'll show you how to turn the `client.download_file()` portion into a pipeline so you can make it reusable in future projects. Further, if you ever need to bundle these models into a Docker image, you will be able to use the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3WIHbwWBTC-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}